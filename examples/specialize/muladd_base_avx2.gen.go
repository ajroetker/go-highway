// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build amd64 && goexperiment.simd

package specialize

import (
	"simd/archsimd"
	"unsafe"

	"github.com/ajroetker/go-highway/hwy"
	"github.com/ajroetker/go-highway/hwy/asm"
)

func BaseMulAdd_avx2_Float16(x []hwy.Float16, y []hwy.Float16, out []hwy.Float16) {
	size := min(len(x), min(len(y), len(out)))
	if size == 0 {
		return
	}
	lanes := 8
	var i int
	for ; i+lanes*4 <= size; i += lanes * 4 {
		vx := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&x[i:][0]))
		vy := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&y[i:][0]))
		vo := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&out[i:][0]))
		vx.MulAdd(vy, vo).StorePtr(unsafe.Pointer(&out[i:][0]))
		vx1 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&x[i+8:][0]))
		vy1 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&y[i+8:][0]))
		vo1 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&out[i+8:][0]))
		vx1.MulAdd(vy1, vo1).StorePtr(unsafe.Pointer(&out[i+8:][0]))
		vx2 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&x[i+16:][0]))
		vy2 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&y[i+16:][0]))
		vo2 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&out[i+16:][0]))
		vx2.MulAdd(vy2, vo2).StorePtr(unsafe.Pointer(&out[i+16:][0]))
		vx3 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&x[i+24:][0]))
		vy3 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&y[i+24:][0]))
		vo3 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&out[i+24:][0]))
		vx3.MulAdd(vy3, vo3).StorePtr(unsafe.Pointer(&out[i+24:][0]))
	}
	if i < size {
		BaseMulAdd_fallback_Float16(x[i:size], y[i:size], out[i:size])
	}
}

func BaseMulAdd_avx2_BFloat16(x []hwy.BFloat16, y []hwy.BFloat16, out []hwy.BFloat16) {
	size := min(len(x), min(len(y), len(out)))
	if size == 0 {
		return
	}
	lanes := 8
	var i int
	for ; i+lanes*4 <= size; i += lanes * 4 {
		vx := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&x[i:][0]))
		vy := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&y[i:][0]))
		vo := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&out[i:][0]))
		vx.MulAdd(vy, vo).StorePtr(unsafe.Pointer(&out[i:][0]))
		vx1 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&x[i+8:][0]))
		vy1 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&y[i+8:][0]))
		vo1 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&out[i+8:][0]))
		vx1.MulAdd(vy1, vo1).StorePtr(unsafe.Pointer(&out[i+8:][0]))
		vx2 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&x[i+16:][0]))
		vy2 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&y[i+16:][0]))
		vo2 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&out[i+16:][0]))
		vx2.MulAdd(vy2, vo2).StorePtr(unsafe.Pointer(&out[i+16:][0]))
		vx3 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&x[i+24:][0]))
		vy3 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&y[i+24:][0]))
		vo3 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&out[i+24:][0]))
		vx3.MulAdd(vy3, vo3).StorePtr(unsafe.Pointer(&out[i+24:][0]))
	}
	if i < size {
		BaseMulAdd_fallback_BFloat16(x[i:size], y[i:size], out[i:size])
	}
}

func BaseMulAdd_avx2(x []float32, y []float32, out []float32) {
	size := min(len(x), min(len(y), len(out)))
	if size == 0 {
		return
	}
	lanes := 8
	var i int
	for ; i+lanes*4 <= size; i += lanes * 4 {
		vx := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&x[i])))
		vy := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&y[i])))
		vo := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&out[i])))
		vx.MulAdd(vy, vo).Store((*[8]float32)(unsafe.Pointer(&out[i])))
		vx1 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&x[i+8])))
		vy1 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&y[i+8])))
		vo1 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&out[i+8])))
		vx1.MulAdd(vy1, vo1).Store((*[8]float32)(unsafe.Pointer(&out[i+8])))
		vx2 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&x[i+16])))
		vy2 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&y[i+16])))
		vo2 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&out[i+16])))
		vx2.MulAdd(vy2, vo2).Store((*[8]float32)(unsafe.Pointer(&out[i+16])))
		vx3 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&x[i+24])))
		vy3 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&y[i+24])))
		vo3 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&out[i+24])))
		vx3.MulAdd(vy3, vo3).Store((*[8]float32)(unsafe.Pointer(&out[i+24])))
	}
	if i < size {
		BaseMulAdd_fallback(x[i:size], y[i:size], out[i:size])
	}
}

func BaseMulAdd_avx2_Float64(x []float64, y []float64, out []float64) {
	size := min(len(x), min(len(y), len(out)))
	if size == 0 {
		return
	}
	lanes := 4
	var i int
	for ; i+lanes*4 <= size; i += lanes * 4 {
		vx := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&x[i])))
		vy := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&y[i])))
		vo := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&out[i])))
		vx.MulAdd(vy, vo).Store((*[4]float64)(unsafe.Pointer(&out[i])))
		vx1 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&x[i+4])))
		vy1 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&y[i+4])))
		vo1 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&out[i+4])))
		vx1.MulAdd(vy1, vo1).Store((*[4]float64)(unsafe.Pointer(&out[i+4])))
		vx2 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&x[i+8])))
		vy2 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&y[i+8])))
		vo2 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&out[i+8])))
		vx2.MulAdd(vy2, vo2).Store((*[4]float64)(unsafe.Pointer(&out[i+8])))
		vx3 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&x[i+12])))
		vy3 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&y[i+12])))
		vo3 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&out[i+12])))
		vx3.MulAdd(vy3, vo3).Store((*[4]float64)(unsafe.Pointer(&out[i+12])))
	}
	if i < size {
		BaseMulAdd_fallback_Float64(x[i:size], y[i:size], out[i:size])
	}
}
