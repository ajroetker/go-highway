// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build amd64 && goexperiment.simd

package matmul

import (
	stdmath "math"
	"simd/archsimd"
	"unsafe"

	"github.com/ajroetker/go-highway/hwy/contrib/math"
)

// Hoisted constants - pre-broadcasted at package init time
var (
	BaseFusedInt4MatMulGELUApprox_AVX2_coeff_f32 = archsimd.BroadcastFloat32x8(float32(1.702))
	BaseFusedInt4MatMulGELU_AVX2_half_f32        = archsimd.BroadcastFloat32x8(float32(0.5))
	BaseFusedInt4MatMulGELU_AVX2_invSqrt2_f32    = archsimd.BroadcastFloat32x8(float32(0.7071067811865476))
	BaseFusedInt4MatMulGELU_AVX2_one_f32         = archsimd.BroadcastFloat32x8(float32(1.0))
	BaseFusedNF4MatMulGELUApprox_AVX2_coeff_f32  = archsimd.BroadcastFloat32x8(float32(1.702))
	BaseFusedNF4MatMulGELU_AVX2_half_f32         = archsimd.BroadcastFloat32x8(float32(0.5))
	BaseFusedNF4MatMulGELU_AVX2_invSqrt2_f32     = archsimd.BroadcastFloat32x8(float32(0.7071067811865476))
	BaseFusedNF4MatMulGELU_AVX2_one_f32          = archsimd.BroadcastFloat32x8(float32(1.0))
)

func BaseFusedNF4MatMulSiLU_avx2(input []float32, packed []uint8, scales []float32, bias []float32, output []float32, M int, K int, N int, groupSize int) {
	if M == 0 || K == 0 || N == 0 {
		return
	}
	numGroups := (N + groupSize - 1) / groupSize
	lanes := 8
	dequantBuf := [8]float32{}
	accBuf := make([]float32, N)
	for m := range M {
		inputRow := input[m*K : (m+1)*K]
		outputRow := output[m*N : (m+1)*N]
		for i := range N {
			accBuf[i] = 0
		}
		for k := range K {
			inputVal := archsimd.BroadcastFloat32x8(inputRow[k])
			baseIdx := k * N
			scaleBase := k * numGroups
			var n int
			for n = 0; n+lanes <= N; n += lanes {
				for lane := range lanes {
					colIdx := n + lane
					weightIdx := baseIdx + colIdx
					packedIdx := weightIdx / 2
					var quantIdx int
					if weightIdx%2 == 0 {
						quantIdx = int(packed[packedIdx] & 0x0F)
					} else {
						quantIdx = int((packed[packedIdx] >> 4) & 0x0F)
					}
					groupIdx := colIdx / groupSize
					scale := scales[scaleBase+groupIdx]
					dequantBuf[lane] = nf4LookupTable[quantIdx] * scale
				}
				weights := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&dequantBuf[0])))
				acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
				acc = inputVal.MulAdd(weights, acc)
				acc.Store((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			}
			for ; n < N; n++ {
				weightIdx := baseIdx + n
				packedIdx := weightIdx / 2
				var quantIdx int
				if weightIdx%2 == 0 {
					quantIdx = int(packed[packedIdx] & 0x0F)
				} else {
					quantIdx = int((packed[packedIdx] >> 4) & 0x0F)
				}
				groupIdx := n / groupSize
				scale := scales[scaleBase+groupIdx]
				accBuf[n] += inputRow[k] * nf4LookupTable[quantIdx] * scale
			}
		}
		var n int
		for n = 0; n+lanes <= N; n += lanes {
			acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			if bias != nil {
				biasVec := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&bias[n])))
				acc = acc.Add(biasVec)
			}
			sig := math.BaseSigmoidVec_avx2(acc)
			acc = acc.Mul(sig)
			acc.Store((*[8]float32)(unsafe.Pointer(&outputRow[n])))
		}
		for ; n < N; n++ {
			sum := accBuf[n]
			if bias != nil {
				sum += bias[n]
			}
			outputRow[n] = sum / (1.0 + float32(stdmath.Exp(float64(-sum))))
		}
	}
}

func BaseFusedNF4MatMulGELU_avx2(input []float32, packed []uint8, scales []float32, bias []float32, output []float32, M int, K int, N int, groupSize int) {
	if M == 0 || K == 0 || N == 0 {
		return
	}
	numGroups := (N + groupSize - 1) / groupSize
	lanes := 8
	dequantBuf := [8]float32{}
	accBuf := make([]float32, N)
	for m := range M {
		inputRow := input[m*K : (m+1)*K]
		outputRow := output[m*N : (m+1)*N]
		for i := range N {
			accBuf[i] = 0
		}
		for k := range K {
			inputVal := archsimd.BroadcastFloat32x8(inputRow[k])
			baseIdx := k * N
			scaleBase := k * numGroups
			var n int
			for n = 0; n+lanes <= N; n += lanes {
				for lane := range lanes {
					colIdx := n + lane
					weightIdx := baseIdx + colIdx
					packedIdx := weightIdx / 2
					var quantIdx int
					if weightIdx%2 == 0 {
						quantIdx = int(packed[packedIdx] & 0x0F)
					} else {
						quantIdx = int((packed[packedIdx] >> 4) & 0x0F)
					}
					groupIdx := colIdx / groupSize
					scale := scales[scaleBase+groupIdx]
					dequantBuf[lane] = nf4LookupTable[quantIdx] * scale
				}
				weights := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&dequantBuf[0])))
				acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
				acc = inputVal.MulAdd(weights, acc)
				acc.Store((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			}
			for ; n < N; n++ {
				weightIdx := baseIdx + n
				packedIdx := weightIdx / 2
				var quantIdx int
				if weightIdx%2 == 0 {
					quantIdx = int(packed[packedIdx] & 0x0F)
				} else {
					quantIdx = int((packed[packedIdx] >> 4) & 0x0F)
				}
				groupIdx := n / groupSize
				scale := scales[scaleBase+groupIdx]
				accBuf[n] += inputRow[k] * nf4LookupTable[quantIdx] * scale
			}
		}
		var n int
		for n = 0; n+lanes <= N; n += lanes {
			acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			if bias != nil {
				biasVec := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&bias[n])))
				acc = acc.Add(biasVec)
			}
			invSqrt2 := BaseFusedNF4MatMulGELU_AVX2_invSqrt2_f32
			half := BaseFusedNF4MatMulGELU_AVX2_half_f32
			one := BaseFusedNF4MatMulGELU_AVX2_one_f32
			scaled := acc.Mul(invSqrt2)
			erfVal := math.BaseErfVec_avx2(scaled)
			acc = acc.Mul(half.Mul(one.Add(erfVal)))
			acc.Store((*[8]float32)(unsafe.Pointer(&outputRow[n])))
		}
		for ; n < N; n++ {
			sum := accBuf[n]
			if bias != nil {
				sum += bias[n]
			}
			outputRow[n] = sum * 0.5 * (1.0 + float32(stdmath.Erf(float64(sum)*0.7071067811865476)))
		}
	}
}

func BaseFusedNF4MatMulGELUApprox_avx2(input []float32, packed []uint8, scales []float32, bias []float32, output []float32, M int, K int, N int, groupSize int) {
	if M == 0 || K == 0 || N == 0 {
		return
	}
	numGroups := (N + groupSize - 1) / groupSize
	lanes := 8
	dequantBuf := [8]float32{}
	accBuf := make([]float32, N)
	for m := range M {
		inputRow := input[m*K : (m+1)*K]
		outputRow := output[m*N : (m+1)*N]
		for i := range N {
			accBuf[i] = 0
		}
		for k := range K {
			inputVal := archsimd.BroadcastFloat32x8(inputRow[k])
			baseIdx := k * N
			scaleBase := k * numGroups
			var n int
			for n = 0; n+lanes <= N; n += lanes {
				for lane := range lanes {
					colIdx := n + lane
					weightIdx := baseIdx + colIdx
					packedIdx := weightIdx / 2
					var quantIdx int
					if weightIdx%2 == 0 {
						quantIdx = int(packed[packedIdx] & 0x0F)
					} else {
						quantIdx = int((packed[packedIdx] >> 4) & 0x0F)
					}
					groupIdx := colIdx / groupSize
					scale := scales[scaleBase+groupIdx]
					dequantBuf[lane] = nf4LookupTable[quantIdx] * scale
				}
				weights := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&dequantBuf[0])))
				acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
				acc = inputVal.MulAdd(weights, acc)
				acc.Store((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			}
			for ; n < N; n++ {
				weightIdx := baseIdx + n
				packedIdx := weightIdx / 2
				var quantIdx int
				if weightIdx%2 == 0 {
					quantIdx = int(packed[packedIdx] & 0x0F)
				} else {
					quantIdx = int((packed[packedIdx] >> 4) & 0x0F)
				}
				groupIdx := n / groupSize
				scale := scales[scaleBase+groupIdx]
				accBuf[n] += inputRow[k] * nf4LookupTable[quantIdx] * scale
			}
		}
		var n int
		for n = 0; n+lanes <= N; n += lanes {
			acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			if bias != nil {
				biasVec := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&bias[n])))
				acc = acc.Add(biasVec)
			}
			coeff := BaseFusedNF4MatMulGELUApprox_AVX2_coeff_f32
			scaled := acc.Mul(coeff)
			sig := math.BaseSigmoidVec_avx2(scaled)
			acc = acc.Mul(sig)
			acc.Store((*[8]float32)(unsafe.Pointer(&outputRow[n])))
		}
		for ; n < N; n++ {
			sum := accBuf[n]
			if bias != nil {
				sum += bias[n]
			}
			outputRow[n] = sum / (1.0 + float32(stdmath.Exp(float64(-1.702*sum))))
		}
	}
}

func BaseFusedNF4MatMulReLU_avx2(input []float32, packed []uint8, scales []float32, bias []float32, output []float32, M int, K int, N int, groupSize int) {
	if M == 0 || K == 0 || N == 0 {
		return
	}
	numGroups := (N + groupSize - 1) / groupSize
	lanes := 8
	dequantBuf := [8]float32{}
	accBuf := make([]float32, N)
	for m := range M {
		inputRow := input[m*K : (m+1)*K]
		outputRow := output[m*N : (m+1)*N]
		for i := range N {
			accBuf[i] = 0
		}
		for k := range K {
			inputVal := archsimd.BroadcastFloat32x8(inputRow[k])
			baseIdx := k * N
			scaleBase := k * numGroups
			var n int
			for n = 0; n+lanes <= N; n += lanes {
				for lane := range lanes {
					colIdx := n + lane
					weightIdx := baseIdx + colIdx
					packedIdx := weightIdx / 2
					var quantIdx int
					if weightIdx%2 == 0 {
						quantIdx = int(packed[packedIdx] & 0x0F)
					} else {
						quantIdx = int((packed[packedIdx] >> 4) & 0x0F)
					}
					groupIdx := colIdx / groupSize
					scale := scales[scaleBase+groupIdx]
					dequantBuf[lane] = nf4LookupTable[quantIdx] * scale
				}
				weights := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&dequantBuf[0])))
				acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
				acc = inputVal.MulAdd(weights, acc)
				acc.Store((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			}
			for ; n < N; n++ {
				weightIdx := baseIdx + n
				packedIdx := weightIdx / 2
				var quantIdx int
				if weightIdx%2 == 0 {
					quantIdx = int(packed[packedIdx] & 0x0F)
				} else {
					quantIdx = int((packed[packedIdx] >> 4) & 0x0F)
				}
				groupIdx := n / groupSize
				scale := scales[scaleBase+groupIdx]
				accBuf[n] += inputRow[k] * nf4LookupTable[quantIdx] * scale
			}
		}
		var n int
		for n = 0; n+lanes <= N; n += lanes {
			acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			if bias != nil {
				biasVec := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&bias[n])))
				acc = acc.Add(biasVec)
			}
			acc = acc.Max(archsimd.BroadcastFloat32x8(0))
			acc.Store((*[8]float32)(unsafe.Pointer(&outputRow[n])))
		}
		for ; n < N; n++ {
			sum := accBuf[n]
			if bias != nil {
				sum += bias[n]
			}
			outputRow[n] = float32(stdmath.Max(0, float64(sum)))
		}
	}
}

func BaseFusedInt4MatMulSiLU_avx2(input []float32, packed []uint8, scales []float32, bias []float32, output []float32, M int, K int, N int, groupSize int) {
	if M == 0 || K == 0 || N == 0 {
		return
	}
	numGroups := (N + groupSize - 1) / groupSize
	lanes := 8
	dequantBuf := [8]float32{}
	accBuf := make([]float32, N)
	for m := range M {
		inputRow := input[m*K : (m+1)*K]
		outputRow := output[m*N : (m+1)*N]
		for i := range N {
			accBuf[i] = 0
		}
		for k := range K {
			inputVal := archsimd.BroadcastFloat32x8(inputRow[k])
			baseIdx := k * N
			scaleBase := k * numGroups
			var n int
			for n = 0; n+lanes <= N; n += lanes {
				for lane := range lanes {
					colIdx := n + lane
					weightIdx := baseIdx + colIdx
					packedIdx := weightIdx / 2
					var unsignedVal int
					if weightIdx%2 == 0 {
						unsignedVal = int(packed[packedIdx] & 0x0F)
					} else {
						unsignedVal = int((packed[packedIdx] >> 4) & 0x0F)
					}
					groupIdx := colIdx / groupSize
					scale := scales[scaleBase+groupIdx]
					dequantBuf[lane] = float32(unsignedVal-8) * scale
				}
				weights := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&dequantBuf[0])))
				acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
				acc = inputVal.MulAdd(weights, acc)
				acc.Store((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			}
			for ; n < N; n++ {
				weightIdx := baseIdx + n
				packedIdx := weightIdx / 2
				var unsignedVal int
				if weightIdx%2 == 0 {
					unsignedVal = int(packed[packedIdx] & 0x0F)
				} else {
					unsignedVal = int((packed[packedIdx] >> 4) & 0x0F)
				}
				groupIdx := n / groupSize
				scale := scales[scaleBase+groupIdx]
				accBuf[n] += inputRow[k] * float32(unsignedVal-8) * scale
			}
		}
		var n int
		for n = 0; n+lanes <= N; n += lanes {
			acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			if bias != nil {
				biasVec := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&bias[n])))
				acc = acc.Add(biasVec)
			}
			sig := math.BaseSigmoidVec_avx2(acc)
			acc = acc.Mul(sig)
			acc.Store((*[8]float32)(unsafe.Pointer(&outputRow[n])))
		}
		for ; n < N; n++ {
			sum := accBuf[n]
			if bias != nil {
				sum += bias[n]
			}
			outputRow[n] = sum / (1.0 + float32(stdmath.Exp(float64(-sum))))
		}
	}
}

func BaseFusedInt4MatMulGELU_avx2(input []float32, packed []uint8, scales []float32, bias []float32, output []float32, M int, K int, N int, groupSize int) {
	if M == 0 || K == 0 || N == 0 {
		return
	}
	numGroups := (N + groupSize - 1) / groupSize
	lanes := 8
	dequantBuf := [8]float32{}
	accBuf := make([]float32, N)
	for m := range M {
		inputRow := input[m*K : (m+1)*K]
		outputRow := output[m*N : (m+1)*N]
		for i := range N {
			accBuf[i] = 0
		}
		for k := range K {
			inputVal := archsimd.BroadcastFloat32x8(inputRow[k])
			baseIdx := k * N
			scaleBase := k * numGroups
			var n int
			for n = 0; n+lanes <= N; n += lanes {
				for lane := range lanes {
					colIdx := n + lane
					weightIdx := baseIdx + colIdx
					packedIdx := weightIdx / 2
					var unsignedVal int
					if weightIdx%2 == 0 {
						unsignedVal = int(packed[packedIdx] & 0x0F)
					} else {
						unsignedVal = int((packed[packedIdx] >> 4) & 0x0F)
					}
					groupIdx := colIdx / groupSize
					scale := scales[scaleBase+groupIdx]
					dequantBuf[lane] = float32(unsignedVal-8) * scale
				}
				weights := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&dequantBuf[0])))
				acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
				acc = inputVal.MulAdd(weights, acc)
				acc.Store((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			}
			for ; n < N; n++ {
				weightIdx := baseIdx + n
				packedIdx := weightIdx / 2
				var unsignedVal int
				if weightIdx%2 == 0 {
					unsignedVal = int(packed[packedIdx] & 0x0F)
				} else {
					unsignedVal = int((packed[packedIdx] >> 4) & 0x0F)
				}
				groupIdx := n / groupSize
				scale := scales[scaleBase+groupIdx]
				accBuf[n] += inputRow[k] * float32(unsignedVal-8) * scale
			}
		}
		var n int
		for n = 0; n+lanes <= N; n += lanes {
			acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			if bias != nil {
				biasVec := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&bias[n])))
				acc = acc.Add(biasVec)
			}
			invSqrt2 := BaseFusedInt4MatMulGELU_AVX2_invSqrt2_f32
			half := BaseFusedInt4MatMulGELU_AVX2_half_f32
			one := BaseFusedInt4MatMulGELU_AVX2_one_f32
			scaled := acc.Mul(invSqrt2)
			erfVal := math.BaseErfVec_avx2(scaled)
			acc = acc.Mul(half.Mul(one.Add(erfVal)))
			acc.Store((*[8]float32)(unsafe.Pointer(&outputRow[n])))
		}
		for ; n < N; n++ {
			sum := accBuf[n]
			if bias != nil {
				sum += bias[n]
			}
			outputRow[n] = sum * 0.5 * (1.0 + float32(stdmath.Erf(float64(sum)*0.7071067811865476)))
		}
	}
}

func BaseFusedInt4MatMulGELUApprox_avx2(input []float32, packed []uint8, scales []float32, bias []float32, output []float32, M int, K int, N int, groupSize int) {
	if M == 0 || K == 0 || N == 0 {
		return
	}
	numGroups := (N + groupSize - 1) / groupSize
	lanes := 8
	dequantBuf := [8]float32{}
	accBuf := make([]float32, N)
	for m := range M {
		inputRow := input[m*K : (m+1)*K]
		outputRow := output[m*N : (m+1)*N]
		for i := range N {
			accBuf[i] = 0
		}
		for k := range K {
			inputVal := archsimd.BroadcastFloat32x8(inputRow[k])
			baseIdx := k * N
			scaleBase := k * numGroups
			var n int
			for n = 0; n+lanes <= N; n += lanes {
				for lane := range lanes {
					colIdx := n + lane
					weightIdx := baseIdx + colIdx
					packedIdx := weightIdx / 2
					var unsignedVal int
					if weightIdx%2 == 0 {
						unsignedVal = int(packed[packedIdx] & 0x0F)
					} else {
						unsignedVal = int((packed[packedIdx] >> 4) & 0x0F)
					}
					groupIdx := colIdx / groupSize
					scale := scales[scaleBase+groupIdx]
					dequantBuf[lane] = float32(unsignedVal-8) * scale
				}
				weights := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&dequantBuf[0])))
				acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
				acc = inputVal.MulAdd(weights, acc)
				acc.Store((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			}
			for ; n < N; n++ {
				weightIdx := baseIdx + n
				packedIdx := weightIdx / 2
				var unsignedVal int
				if weightIdx%2 == 0 {
					unsignedVal = int(packed[packedIdx] & 0x0F)
				} else {
					unsignedVal = int((packed[packedIdx] >> 4) & 0x0F)
				}
				groupIdx := n / groupSize
				scale := scales[scaleBase+groupIdx]
				accBuf[n] += inputRow[k] * float32(unsignedVal-8) * scale
			}
		}
		var n int
		for n = 0; n+lanes <= N; n += lanes {
			acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			if bias != nil {
				biasVec := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&bias[n])))
				acc = acc.Add(biasVec)
			}
			coeff := BaseFusedInt4MatMulGELUApprox_AVX2_coeff_f32
			scaled := acc.Mul(coeff)
			sig := math.BaseSigmoidVec_avx2(scaled)
			acc = acc.Mul(sig)
			acc.Store((*[8]float32)(unsafe.Pointer(&outputRow[n])))
		}
		for ; n < N; n++ {
			sum := accBuf[n]
			if bias != nil {
				sum += bias[n]
			}
			outputRow[n] = sum / (1.0 + float32(stdmath.Exp(float64(-1.702*sum))))
		}
	}
}

func BaseFusedInt4MatMulReLU_avx2(input []float32, packed []uint8, scales []float32, bias []float32, output []float32, M int, K int, N int, groupSize int) {
	if M == 0 || K == 0 || N == 0 {
		return
	}
	numGroups := (N + groupSize - 1) / groupSize
	lanes := 8
	dequantBuf := [8]float32{}
	accBuf := make([]float32, N)
	for m := range M {
		inputRow := input[m*K : (m+1)*K]
		outputRow := output[m*N : (m+1)*N]
		for i := range N {
			accBuf[i] = 0
		}
		for k := range K {
			inputVal := archsimd.BroadcastFloat32x8(inputRow[k])
			baseIdx := k * N
			scaleBase := k * numGroups
			var n int
			for n = 0; n+lanes <= N; n += lanes {
				for lane := range lanes {
					colIdx := n + lane
					weightIdx := baseIdx + colIdx
					packedIdx := weightIdx / 2
					var unsignedVal int
					if weightIdx%2 == 0 {
						unsignedVal = int(packed[packedIdx] & 0x0F)
					} else {
						unsignedVal = int((packed[packedIdx] >> 4) & 0x0F)
					}
					groupIdx := colIdx / groupSize
					scale := scales[scaleBase+groupIdx]
					dequantBuf[lane] = float32(unsignedVal-8) * scale
				}
				weights := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&dequantBuf[0])))
				acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
				acc = inputVal.MulAdd(weights, acc)
				acc.Store((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			}
			for ; n < N; n++ {
				weightIdx := baseIdx + n
				packedIdx := weightIdx / 2
				var unsignedVal int
				if weightIdx%2 == 0 {
					unsignedVal = int(packed[packedIdx] & 0x0F)
				} else {
					unsignedVal = int((packed[packedIdx] >> 4) & 0x0F)
				}
				groupIdx := n / groupSize
				scale := scales[scaleBase+groupIdx]
				accBuf[n] += inputRow[k] * float32(unsignedVal-8) * scale
			}
		}
		var n int
		for n = 0; n+lanes <= N; n += lanes {
			acc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&accBuf[n])))
			if bias != nil {
				biasVec := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&bias[n])))
				acc = acc.Add(biasVec)
			}
			acc = acc.Max(archsimd.BroadcastFloat32x8(0))
			acc.Store((*[8]float32)(unsafe.Pointer(&outputRow[n])))
		}
		for ; n < N; n++ {
			sum := accBuf[n]
			if bias != nil {
				sum += bias[n]
			}
			outputRow[n] = float32(stdmath.Max(0, float64(sum)))
		}
	}
}

func BaseFusedNF4MatMulSwiGLU_avx2(input []float32, gatePacked []uint8, gateScales []float32, upPacked []uint8, upScales []float32, output []float32, M int, K int, N int, groupSize int) {
	if M == 0 || K == 0 || N == 0 {
		return
	}
	numGroups := (N + groupSize - 1) / groupSize
	lanes := 8
	gateBuf := [8]float32{}
	upBuf := [8]float32{}
	gateAccBuf := make([]float32, N)
	upAccBuf := make([]float32, N)
	for m := range M {
		inputRow := input[m*K : (m+1)*K]
		outputRow := output[m*N : (m+1)*N]
		for i := range N {
			gateAccBuf[i] = 0
			upAccBuf[i] = 0
		}
		for k := range K {
			inputVal := archsimd.BroadcastFloat32x8(inputRow[k])
			baseIdx := k * N
			scaleBase := k * numGroups
			var n int
			for n = 0; n+lanes <= N; n += lanes {
				for lane := range lanes {
					colIdx := n + lane
					weightIdx := baseIdx + colIdx
					packedIdx := weightIdx / 2
					groupIdx := colIdx / groupSize
					var gateQuantIdx int
					if weightIdx%2 == 0 {
						gateQuantIdx = int(gatePacked[packedIdx] & 0x0F)
					} else {
						gateQuantIdx = int((gatePacked[packedIdx] >> 4) & 0x0F)
					}
					gateScale := gateScales[scaleBase+groupIdx]
					gateBuf[lane] = nf4LookupTable[gateQuantIdx] * gateScale
					var upQuantIdx int
					if weightIdx%2 == 0 {
						upQuantIdx = int(upPacked[packedIdx] & 0x0F)
					} else {
						upQuantIdx = int((upPacked[packedIdx] >> 4) & 0x0F)
					}
					upScale := upScales[scaleBase+groupIdx]
					upBuf[lane] = nf4LookupTable[upQuantIdx] * upScale
				}
				gateWeights := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&gateBuf[0])))
				upWeights := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&upBuf[0])))
				gateAcc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&gateAccBuf[n])))
				upAcc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&upAccBuf[n])))
				gateAcc = inputVal.MulAdd(gateWeights, gateAcc)
				upAcc = inputVal.MulAdd(upWeights, upAcc)
				gateAcc.Store((*[8]float32)(unsafe.Pointer(&gateAccBuf[n])))
				upAcc.Store((*[8]float32)(unsafe.Pointer(&upAccBuf[n])))
			}
			for ; n < N; n++ {
				weightIdx := baseIdx + n
				packedIdx := weightIdx / 2
				groupIdx := n / groupSize
				var gateQuantIdx int
				if weightIdx%2 == 0 {
					gateQuantIdx = int(gatePacked[packedIdx] & 0x0F)
				} else {
					gateQuantIdx = int((gatePacked[packedIdx] >> 4) & 0x0F)
				}
				gateScale := gateScales[scaleBase+groupIdx]
				gateAccBuf[n] += inputRow[k] * nf4LookupTable[gateQuantIdx] * gateScale
				var upQuantIdx int
				if weightIdx%2 == 0 {
					upQuantIdx = int(upPacked[packedIdx] & 0x0F)
				} else {
					upQuantIdx = int((upPacked[packedIdx] >> 4) & 0x0F)
				}
				upScale := upScales[scaleBase+groupIdx]
				upAccBuf[n] += inputRow[k] * nf4LookupTable[upQuantIdx] * upScale
			}
		}
		var n int
		for n = 0; n+lanes <= N; n += lanes {
			gateAcc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&gateAccBuf[n])))
			upAcc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&upAccBuf[n])))
			gateSilu := gateAcc.Mul(math.BaseSigmoidVec_avx2(gateAcc))
			result := gateSilu.Mul(upAcc)
			result.Store((*[8]float32)(unsafe.Pointer(&outputRow[n])))
		}
		for ; n < N; n++ {
			gateSum := gateAccBuf[n]
			upSum := upAccBuf[n]
			gateSilu := gateSum / (1.0 + float32(stdmath.Exp(float64(-gateSum))))
			outputRow[n] = gateSilu * upSum
		}
	}
}

func BaseFusedInt4MatMulSwiGLU_avx2(input []float32, gatePacked []uint8, gateScales []float32, upPacked []uint8, upScales []float32, output []float32, M int, K int, N int, groupSize int) {
	if M == 0 || K == 0 || N == 0 {
		return
	}
	numGroups := (N + groupSize - 1) / groupSize
	lanes := 8
	gateBuf := [8]float32{}
	upBuf := [8]float32{}
	gateAccBuf := make([]float32, N)
	upAccBuf := make([]float32, N)
	for m := range M {
		inputRow := input[m*K : (m+1)*K]
		outputRow := output[m*N : (m+1)*N]
		for i := range N {
			gateAccBuf[i] = 0
			upAccBuf[i] = 0
		}
		for k := range K {
			inputVal := archsimd.BroadcastFloat32x8(inputRow[k])
			baseIdx := k * N
			scaleBase := k * numGroups
			var n int
			for n = 0; n+lanes <= N; n += lanes {
				for lane := range lanes {
					colIdx := n + lane
					weightIdx := baseIdx + colIdx
					packedIdx := weightIdx / 2
					groupIdx := colIdx / groupSize
					var gateUnsigned int
					if weightIdx%2 == 0 {
						gateUnsigned = int(gatePacked[packedIdx] & 0x0F)
					} else {
						gateUnsigned = int((gatePacked[packedIdx] >> 4) & 0x0F)
					}
					gateScale := gateScales[scaleBase+groupIdx]
					gateBuf[lane] = float32(gateUnsigned-8) * gateScale
					var upUnsigned int
					if weightIdx%2 == 0 {
						upUnsigned = int(upPacked[packedIdx] & 0x0F)
					} else {
						upUnsigned = int((upPacked[packedIdx] >> 4) & 0x0F)
					}
					upScale := upScales[scaleBase+groupIdx]
					upBuf[lane] = float32(upUnsigned-8) * upScale
				}
				gateWeights := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&gateBuf[0])))
				upWeights := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&upBuf[0])))
				gateAcc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&gateAccBuf[n])))
				upAcc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&upAccBuf[n])))
				gateAcc = inputVal.MulAdd(gateWeights, gateAcc)
				upAcc = inputVal.MulAdd(upWeights, upAcc)
				gateAcc.Store((*[8]float32)(unsafe.Pointer(&gateAccBuf[n])))
				upAcc.Store((*[8]float32)(unsafe.Pointer(&upAccBuf[n])))
			}
			for ; n < N; n++ {
				weightIdx := baseIdx + n
				packedIdx := weightIdx / 2
				groupIdx := n / groupSize
				var gateUnsigned int
				if weightIdx%2 == 0 {
					gateUnsigned = int(gatePacked[packedIdx] & 0x0F)
				} else {
					gateUnsigned = int((gatePacked[packedIdx] >> 4) & 0x0F)
				}
				gateScale := gateScales[scaleBase+groupIdx]
				gateAccBuf[n] += inputRow[k] * float32(gateUnsigned-8) * gateScale
				var upUnsigned int
				if weightIdx%2 == 0 {
					upUnsigned = int(upPacked[packedIdx] & 0x0F)
				} else {
					upUnsigned = int((upPacked[packedIdx] >> 4) & 0x0F)
				}
				upScale := upScales[scaleBase+groupIdx]
				upAccBuf[n] += inputRow[k] * float32(upUnsigned-8) * upScale
			}
		}
		var n int
		for n = 0; n+lanes <= N; n += lanes {
			gateAcc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&gateAccBuf[n])))
			upAcc := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&upAccBuf[n])))
			gateSilu := gateAcc.Mul(math.BaseSigmoidVec_avx2(gateAcc))
			result := gateSilu.Mul(upAcc)
			result.Store((*[8]float32)(unsafe.Pointer(&outputRow[n])))
		}
		for ; n < N; n++ {
			gateSum := gateAccBuf[n]
			upSum := upAccBuf[n]
			gateSilu := gateSum / (1.0 + float32(stdmath.Exp(float64(-gateSum))))
			outputRow[n] = gateSilu * upSum
		}
	}
}
