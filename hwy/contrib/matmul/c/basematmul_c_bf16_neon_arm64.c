// Generated by hwygen -c (AST translator). DO NOT EDIT.
// BaseMatMul for NEON hwy.BFloat16

#ifndef GOAT_PARSER
#include <arm_neon.h>
#endif

#ifndef GOAT_PARSER
static inline float32x4_t bf16_promote_lo(bfloat16x8_t v) {
    uint16x4_t lo = vget_low_u16(vreinterpretq_u16_bf16(v));
    uint32x4_t wide = vshll_n_u16(lo, 16);
    return vreinterpretq_f32_u32(wide);
}

static inline float32x4_t bf16_promote_hi(bfloat16x8_t v) {
    uint16x4_t hi = vget_high_u16(vreinterpretq_u16_bf16(v));
    uint32x4_t wide = vshll_n_u16(hi, 16);
    return vreinterpretq_f32_u32(wide);
}

static inline uint16x4_t bf16_demote_half(float32x4_t v) {
    uint32x4_t bits = vreinterpretq_u32_f32(v);
    uint32x4_t lsb = vshrq_n_u32(bits, 16);
    lsb = vandq_u32(lsb, vdupq_n_u32(1));
    uint32x4_t bias = vaddq_u32(vdupq_n_u32(0x7FFF), lsb);
    bits = vaddq_u32(bits, bias);
    return vshrn_n_u32(bits, 16);
}

static inline bfloat16x8_t bf16_combine(uint16x4_t lo, uint16x4_t hi) {
    return vreinterpretq_bf16_u16(vcombine_u16(lo, hi));
}

static inline bfloat16x8_t bf16_add_q(bfloat16x8_t a, bfloat16x8_t b) {
    float32x4_t a_lo = bf16_promote_lo(a), a_hi = bf16_promote_hi(a);
    float32x4_t b_lo = bf16_promote_lo(b), b_hi = bf16_promote_hi(b);
    return bf16_combine(bf16_demote_half(vaddq_f32(a_lo, b_lo)),
                        bf16_demote_half(vaddq_f32(a_hi, b_hi)));
}

static inline bfloat16x8_t bf16_sub_q(bfloat16x8_t a, bfloat16x8_t b) {
    float32x4_t a_lo = bf16_promote_lo(a), a_hi = bf16_promote_hi(a);
    float32x4_t b_lo = bf16_promote_lo(b), b_hi = bf16_promote_hi(b);
    return bf16_combine(bf16_demote_half(vsubq_f32(a_lo, b_lo)),
                        bf16_demote_half(vsubq_f32(a_hi, b_hi)));
}

static inline bfloat16x8_t bf16_mul_q(bfloat16x8_t a, bfloat16x8_t b) {
    float32x4_t a_lo = bf16_promote_lo(a), a_hi = bf16_promote_hi(a);
    float32x4_t b_lo = bf16_promote_lo(b), b_hi = bf16_promote_hi(b);
    return bf16_combine(bf16_demote_half(vmulq_f32(a_lo, b_lo)),
                        bf16_demote_half(vmulq_f32(a_hi, b_hi)));
}

static inline bfloat16x8_t bf16_div_q(bfloat16x8_t a, bfloat16x8_t b) {
    float32x4_t a_lo = bf16_promote_lo(a), a_hi = bf16_promote_hi(a);
    float32x4_t b_lo = bf16_promote_lo(b), b_hi = bf16_promote_hi(b);
    return bf16_combine(bf16_demote_half(vdivq_f32(a_lo, b_lo)),
                        bf16_demote_half(vdivq_f32(a_hi, b_hi)));
}

static inline bfloat16x8_t bf16_min_q(bfloat16x8_t a, bfloat16x8_t b) {
    float32x4_t a_lo = bf16_promote_lo(a), a_hi = bf16_promote_hi(a);
    float32x4_t b_lo = bf16_promote_lo(b), b_hi = bf16_promote_hi(b);
    return bf16_combine(bf16_demote_half(vminq_f32(a_lo, b_lo)),
                        bf16_demote_half(vminq_f32(a_hi, b_hi)));
}

static inline bfloat16x8_t bf16_max_q(bfloat16x8_t a, bfloat16x8_t b) {
    float32x4_t a_lo = bf16_promote_lo(a), a_hi = bf16_promote_hi(a);
    float32x4_t b_lo = bf16_promote_lo(b), b_hi = bf16_promote_hi(b);
    return bf16_combine(bf16_demote_half(vmaxq_f32(a_lo, b_lo)),
                        bf16_demote_half(vmaxq_f32(a_hi, b_hi)));
}

static inline bfloat16x8_t bf16_fma_q(bfloat16x8_t acc, bfloat16x8_t a, bfloat16x8_t b) {
    float32x4_t acc_lo = bf16_promote_lo(acc), acc_hi = bf16_promote_hi(acc);
    float32x4_t a_lo = bf16_promote_lo(a), a_hi = bf16_promote_hi(a);
    float32x4_t b_lo = bf16_promote_lo(b), b_hi = bf16_promote_hi(b);
    return bf16_combine(bf16_demote_half(vfmaq_f32(acc_lo, a_lo, b_lo)),
                        bf16_demote_half(vfmaq_f32(acc_hi, a_hi, b_hi)));
}

static inline bfloat16x8_t bf16_neg_q(bfloat16x8_t v) {
    uint16x8_t bits = vreinterpretq_u16_bf16(v);
    bits = veorq_u16(bits, vdupq_n_u16(0x8000));
    return vreinterpretq_bf16_u16(bits);
}

static inline bfloat16x8_t bf16_abs_q(bfloat16x8_t v) {
    uint16x8_t bits = vreinterpretq_u16_bf16(v);
    bits = vandq_u16(bits, vdupq_n_u16(0x7FFF));
    return vreinterpretq_bf16_u16(bits);
}

static inline bfloat16x8_t bf16_sqrt_q(bfloat16x8_t v) {
    float32x4_t lo = bf16_promote_lo(v), hi = bf16_promote_hi(v);
    return bf16_combine(bf16_demote_half(vsqrtq_f32(lo)),
                        bf16_demote_half(vsqrtq_f32(hi)));
}

static inline bfloat16x8_t bf16_zero_q(void) {
    return vreinterpretq_bf16_u16(vdupq_n_u16(0));
}

static inline bfloat16x8_t bf16_dup_q(unsigned short val) {
    return vreinterpretq_bf16_u16(vdupq_n_u16(val));
}

static inline float bf16_reducesum_q(bfloat16x8_t v) {
    float32x4_t lo = bf16_promote_lo(v);
    float32x4_t hi = bf16_promote_hi(v);
    return vaddvq_f32(vaddq_f32(lo, hi));
}

static inline float bf16_reducemin_q(bfloat16x8_t v) {
    float32x4_t lo = bf16_promote_lo(v);
    float32x4_t hi = bf16_promote_hi(v);
    return vminvq_f32(vminq_f32(lo, hi));
}

static inline float bf16_reducemax_q(bfloat16x8_t v) {
    float32x4_t lo = bf16_promote_lo(v);
    float32x4_t hi = bf16_promote_hi(v);
    return vmaxvq_f32(vmaxq_f32(lo, hi));
}

static inline uint16x8_t bf16_lt_q(bfloat16x8_t a, bfloat16x8_t b) {
    uint32x4_t m_lo = vcltq_f32(bf16_promote_lo(a), bf16_promote_lo(b));
    uint32x4_t m_hi = vcltq_f32(bf16_promote_hi(a), bf16_promote_hi(b));
    return vcombine_u16(vmovn_u32(m_lo), vmovn_u32(m_hi));
}

static inline uint16x8_t bf16_eq_q(bfloat16x8_t a, bfloat16x8_t b) {
    uint32x4_t m_lo = vceqq_f32(bf16_promote_lo(a), bf16_promote_lo(b));
    uint32x4_t m_hi = vceqq_f32(bf16_promote_hi(a), bf16_promote_hi(b));
    return vcombine_u16(vmovn_u32(m_lo), vmovn_u32(m_hi));
}

static inline uint16x8_t bf16_gt_q(bfloat16x8_t a, bfloat16x8_t b) {
    uint32x4_t m_lo = vcgtq_f32(bf16_promote_lo(a), bf16_promote_lo(b));
    uint32x4_t m_hi = vcgtq_f32(bf16_promote_hi(a), bf16_promote_hi(b));
    return vcombine_u16(vmovn_u32(m_lo), vmovn_u32(m_hi));
}

static inline bfloat16x8_t bf16_ifelse_q(uint16x8_t mask, bfloat16x8_t yes, bfloat16x8_t no) {
    return vreinterpretq_bf16_u16(vbslq_u16(mask,
        vreinterpretq_u16_bf16(yes), vreinterpretq_u16_bf16(no)));
}

static inline float bf16_scalar_to_f32(unsigned short v) {
    unsigned int bits = (unsigned int)v << 16;
    float f;
    __builtin_memcpy(&f, &bits, 4);
    return f;
}

static inline unsigned short f32_scalar_to_bf16(float f) {
    unsigned int bits;
    __builtin_memcpy(&bits, &f, 4);
    unsigned int lsb = (bits >> 16) & 1;
    bits += 0x7FFF + lsb;
    return (unsigned short)(bits >> 16);
}

static inline float32x4_t _v_exp_f32(float32x4_t x) {
    float32x4_t invLn2 = vdupq_n_f32(1.44269504088896341f);
    float32x4_t ln2Hi = vdupq_n_f32(0.693359375f);
    float32x4_t ln2Lo = vdupq_n_f32(-2.12194440e-4f);
    float32x4_t overflow = vdupq_n_f32(88.72283905206835f);
    float32x4_t underflow = vdupq_n_f32(-87.33654475055310f);
    float32x4_t c1 = vdupq_n_f32(1.0f);
    float32x4_t c2 = vdupq_n_f32(0.5f);
    float32x4_t c3 = vdupq_n_f32(0.16666666666666666f);
    float32x4_t c4 = vdupq_n_f32(0.041666666666666664f);
    float32x4_t c5 = vdupq_n_f32(0.008333333333333333f);
    float32x4_t c6 = vdupq_n_f32(0.001388888888888889f);
    int32x4_t bias = vdupq_n_s32(127);
    float32x4_t zero = vdupq_n_f32(0.0f);
    float32x4_t inf_val = vdupq_n_f32(1.0f / 0.0f);
    uint32x4_t over = vcgtq_f32(x, overflow);
    uint32x4_t under = vcltq_f32(x, underflow);
    float32x4_t kf = vrndnq_f32(vmulq_f32(x, invLn2));
    float32x4_t r = vsubq_f32(x, vmulq_f32(kf, ln2Hi));
    r = vsubq_f32(r, vmulq_f32(kf, ln2Lo));
    float32x4_t ep = vfmaq_f32(c5, c6, r);
    ep = vfmaq_f32(c4, ep, r);
    ep = vfmaq_f32(c3, ep, r);
    ep = vfmaq_f32(c2, ep, r);
    ep = vfmaq_f32(c1, ep, r);
    ep = vfmaq_f32(c1, ep, r);
    int32x4_t ki = vcvtnq_s32_f32(kf);
    int32x4_t scale_bits = vshlq_n_s32(vaddq_s32(ki, bias), 23);
    float32x4_t scale = vreinterpretq_f32_s32(scale_bits);
    float32x4_t result = vmulq_f32(ep, scale);
    result = vbslq_f32(over, inf_val, result);
    result = vbslq_f32(under, zero, result);
    return result;
}

static inline float32x4_t _v_sigmoid_f32(float32x4_t x) {
    float32x4_t one = vdupq_n_f32(1.0f);
    float32x4_t exp_neg = _v_exp_f32(vnegq_f32(x));
    return vdivq_f32(one, vaddq_f32(one, exp_neg));
}

static inline float32x4_t _v_erf_f32(float32x4_t x) {
    float32x4_t zero = vdupq_n_f32(0.0f);
    float32x4_t one = vdupq_n_f32(1.0f);
    float32x4_t abs_x = vabsq_f32(x);
    uint32x4_t neg_mask = vcltq_f32(x, zero);
    float32x4_t sign = vbslq_f32(neg_mask, vdupq_n_f32(-1.0f), one);
    float32x4_t t = vdivq_f32(one, vfmaq_f32(one, vdupq_n_f32(0.3275911f), abs_x));
    float32x4_t t2 = vmulq_f32(t, t);
    float32x4_t t3 = vmulq_f32(t2, t);
    float32x4_t t4 = vmulq_f32(t3, t);
    float32x4_t t5 = vmulq_f32(t4, t);
    float32x4_t poly = vmulq_f32(vdupq_n_f32(0.254829592f), t);
    poly = vfmaq_f32(poly, vdupq_n_f32(-0.284496736f), t2);
    poly = vfmaq_f32(poly, vdupq_n_f32(1.421413741f), t3);
    poly = vfmaq_f32(poly, vdupq_n_f32(-1.453152027f), t4);
    poly = vfmaq_f32(poly, vdupq_n_f32(1.061405429f), t5);
    float32x4_t exp_neg_x2 = _v_exp_f32(vnegq_f32(vmulq_f32(abs_x, abs_x)));
    float32x4_t result = vsubq_f32(one, vmulq_f32(poly, exp_neg_x2));
    return vmulq_f32(sign, result);
}

static inline float32x4_t _v_log_f32(float32x4_t x) {
    float32x4_t one = vdupq_n_f32(1.0f);
    float32x4_t ln2 = vdupq_n_f32(0.6931471805599453f);
    /* Extract exponent: e = ((bits >> 23) & 0xFF) - 127 */
    int32x4_t bits = vreinterpretq_s32_f32(x);
    int32x4_t exp_i = vsubq_s32(vandq_s32(vshrq_n_s32(bits, 23), vdupq_n_s32(0xFF)), vdupq_n_s32(127));
    float32x4_t e = vcvtq_f32_s32(exp_i);
    /* Normalize mantissa to [1,2): m = (bits & 0x7FFFFF) | 0x3F800000 */
    int32x4_t m_bits = vorrq_s32(vandq_s32(bits, vdupq_n_s32(0x007FFFFF)), vdupq_n_s32(0x3F800000));
    float32x4_t m = vreinterpretq_f32_s32(m_bits);
    /* Polynomial for log(m), m in [1,2): use (m-1) as argument */
    float32x4_t f = vsubq_f32(m, one);
    /* Minimax polynomial for log(1+f), f in [0,1) */
    float32x4_t p = vdupq_n_f32(-0.2401093292f);
    p = vfmaq_f32(vdupq_n_f32(0.3317990258f), p, f);
    p = vfmaq_f32(vdupq_n_f32(-0.4998741238f), p, f);
    p = vfmaq_f32(vdupq_n_f32(0.9999964239f), p, f);
    /* result = p + e * ln(2) */
    return vfmaq_f32(p, e, ln2);
}

static inline float _s_exp_f32(float x) {
    if (x > 88.0f) return 1.0f / 0.0f;
    if (x < -88.0f) return 0.0f;
    float kf = __builtin_roundf(x * 1.44269504088896341f);
    float r = x - kf * 0.693359375f;
    r = r - kf * (-2.12194440e-4f);
    float ep = (1.0f/720.0f) * r + (1.0f/120.0f);
    ep = ep * r + (1.0f/24.0f);
    ep = ep * r + (1.0f/6.0f);
    ep = ep * r + 0.5f;
    ep = ep * r + 1.0f;
    ep = ep * r + 1.0f;
    int ki = (int)kf;
    unsigned int bits = (unsigned int)(ki + 127) << 23;
    float scale;
    __builtin_memcpy(&scale, &bits, 4);
    return ep * scale;
}

static inline float _s_sigmoid_f32(float x) {
    return 1.0f / (1.0f + _s_exp_f32(-x));
}

static inline float _s_erf_f32(float x) {
    float sign = 1.0f;
    float ax = x;
    if (x < 0.0f) { sign = -1.0f; ax = -x; }
    float t = 1.0f / (1.0f + 0.3275911f * ax);
    float t2 = t * t;
    float t3 = t2 * t;
    float t4 = t3 * t;
    float t5 = t4 * t;
    float y = 1.0f - (0.254829592f * t - 0.284496736f * t2 +
        1.421413741f * t3 - 1.453152027f * t4 + 1.061405429f * t5) *
        _s_exp_f32(-ax * ax);
    return sign * y;
}

static inline float _s_log_f32(float x) {
    unsigned int bits;
    __builtin_memcpy(&bits, &x, 4);
    int exp_i = (int)((bits >> 23) & 0xFF) - 127;
    float e = (float)exp_i;
    unsigned int m_bits = (bits & 0x007FFFFF) | 0x3F800000;
    float m;
    __builtin_memcpy(&m, &m_bits, 4);
    float f = m - 1.0f;
    float p = -0.2401093292f;
    p = p * f + 0.3317990258f;
    p = p * f + -0.4998741238f;
    p = p * f + 0.9999964239f;
    p = p * f;
    return p + e * 0.6931471805599453f;
}

static inline double _s_exp_f64(double x) {
    if (x > 709.0) return 1.0 / 0.0;
    if (x < -709.0) return 0.0;
    double kf = __builtin_round(x * 1.4426950408889634);
    double r = x - kf * 6.93147180369123816490e-01;
    r = r - kf * 1.90821492927058500170e-10;
    double ep = (1.0/479001600.0) * r + (1.0/39916800.0);
    ep = ep * r + (1.0/3628800.0);
    ep = ep * r + (1.0/362880.0);
    ep = ep * r + (1.0/40320.0);
    ep = ep * r + (1.0/5040.0);
    ep = ep * r + (1.0/720.0);
    ep = ep * r + (1.0/120.0);
    ep = ep * r + (1.0/24.0);
    ep = ep * r + (1.0/6.0);
    ep = ep * r + 0.5;
    ep = ep * r + 1.0;
    ep = ep * r + 1.0;
    long ki = (long)kf;
    unsigned long bits = (unsigned long)(ki + 1023) << 52;
    double scale;
    __builtin_memcpy(&scale, &bits, 8);
    return ep * scale;
}

static inline double _s_sigmoid_f64(double x) {
    return 1.0 / (1.0 + _s_exp_f64(-x));
}

static inline double _s_erf_f64(double x) {
    double sign = 1.0;
    double ax = x;
    if (x < 0.0) { sign = -1.0; ax = -x; }
    double t = 1.0 / (1.0 + 0.3275911 * ax);
    double t2 = t * t;
    double t3 = t2 * t;
    double t4 = t3 * t;
    double t5 = t4 * t;
    double y = 1.0 - (0.254829592 * t - 0.284496736 * t2 +
        1.421413741 * t3 - 1.453152027 * t4 + 1.061405429 * t5) *
        _s_exp_f64(-ax * ax);
    return sign * y;
}

static inline double _s_log_f64(double x) {
    unsigned long bits;
    __builtin_memcpy(&bits, &x, 8);
    long exp_i = (long)((bits >> 52) & 0x7FF) - 1023;
    double e = (double)exp_i;
    unsigned long m_bits = (bits & 0x000FFFFFFFFFFFFF) | 0x3FF0000000000000;
    double m;
    __builtin_memcpy(&m, &m_bits, 8);
    double f = m - 1.0;
    double p = 0.1484794514;
    p = p * f + -0.1792383373;
    p = p * f + 0.2211827839;
    p = p * f + -0.2857142857;
    p = p * f + 0.3999999999;
    p = p * f + -0.4999999999;
    p = p * f + 0.9999999999;
    p = p * f;
    return p + e * 0.6931471805599453;
}

#endif

void matmul_c_bf16_neon(unsigned short *a, unsigned short *b, unsigned short *c, long *pm, long *pn, long *pk, long *plen_a, long *plen_b, long *plen_c) {
    long k = *pk;
    long len_a = *plen_a;
    long len_b = *plen_b;
    long len_c = *plen_c;
    long m = *pm;
    long n = *pn;
    #pragma clang loop vectorize(disable) interleave(disable)
    for (long i = 0; i < m; i++) {
        unsigned short *cRow = c + i * n;
        bfloat16x8_t vZero = bf16_zero_q();
        long lanes = 8;
        long j = 0;
        #pragma clang loop vectorize(disable) interleave(disable)
        for (j = 0; j + lanes <= n; j += lanes) {
            vst1q_bf16((bfloat16_t*)(cRow + j), vZero);
        }
        #pragma clang loop vectorize(disable) interleave(disable)
        for (; j < n; j++) {
            cRow[j] = f32_scalar_to_bf16(0);
        }
        #pragma clang loop vectorize(disable) interleave(disable)
        for (long p = 0; p < k; p++) {
            unsigned short aip = a[i * k + p];
            bfloat16x8_t vA = bf16_dup_q(aip);
            unsigned short *bRow = b + p * n;
            #pragma clang loop vectorize(disable) interleave(disable)
            for (j = 0; j + lanes <= n; j += lanes) {
                bfloat16x8_t vB = vld1q_bf16((bfloat16_t*)(bRow + j));
                bfloat16x8_t vC = vld1q_bf16((bfloat16_t*)(cRow + j));
                vC = bf16_fma_q(vC, vA, vB);
                vst1q_bf16((bfloat16_t*)(cRow + j), vC);
            }
            #pragma clang loop vectorize(disable) interleave(disable)
            for (; j < n; j++) {
                cRow[j] = f32_scalar_to_bf16(bf16_scalar_to_f32(cRow[j]) + bf16_scalar_to_f32(aip) * bf16_scalar_to_f32(bRow[j]));
            }
        }
    }
}
